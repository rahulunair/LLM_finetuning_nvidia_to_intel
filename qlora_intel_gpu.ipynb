{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94ae4f8d-cccc-48b9-bf81-b822668e2501",
   "metadata": {},
   "source": [
    "### MVP of LLM finetuning and Inferene using QLoRA in 4 bits on Intel GPUs\n",
    "\n",
    "\n",
    "In this notebook, we load an LLM in 4bit (\"NousResearch/Nous-Hermes-Llama-2-7b\") and train it on Intel Developer Cloud ([IDC](https://cloud.intel.com)) using the PEFT library from Hugging Face 🤗.\n",
    "\n",
    "A reference to run this same training on Nvidia GPUs can be found [here](https://colab.research.google.com/drive/1H1SHcmYrHiHtAIJwMXT4E7dhXDLqvGtr?usp=sharing). For an extensive tutorial on finetuning an LLM using this approach on Intel® Data Center GPU Max 1100, check out the **LLM Finetuning notebook** under **\"GenAI Essentials\"** on IDC.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b7a878-0b8f-4e8b-a089-7617c5e1bc2f",
   "metadata": {},
   "source": [
    " **Executive Summary: Changes required to port the same workload from Nvidia to Intel GPUs:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9551d76-13bd-4cbc-882f-9ff8e3841d5b",
   "metadata": {},
   "source": [
    "<style>\n",
    ".custom-table {\n",
    "    border-collapse: collapse;\n",
    "    width: 100%;\n",
    "}\n",
    ".custom-table th, .custom-table td {\n",
    "    border: 1px solid black;\n",
    "    padding: 8px;\n",
    "    text-align: left;\n",
    "}\n",
    ".custom-table th {\n",
    "    background-color: #f2f2f2;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<table class=\"custom-table\">\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <th><b>Nvidia A100 Implementation</b></th>\n",
    "        <th><b>Intel PVC 1100 Implementation</b></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><b>Packages required:</b></td>\n",
    "                <td>pip install -q -U bitsandbytes<br></td>\n",
    "        <td>pip install -q --pre -U bigdl-llm[xpu] -f https://developer.intel.com/ipex-whl-stable-xpu<br></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><b>Import Required Libraries</b></td>\n",
    "        <td>from transformers import BitsAndBytesConfig<br>from transformers import AutoModelForCausalLM<br>from peft import get_peft_model, prepare_model_for_kbit_training</td>\n",
    "        <td>import intel_extension_for_pytorch as ipex<br>from bigdl.llm.transformers import AutoModelForCausalLM<br>from bigdl.llm.transformers.qlora import get_peft_model, prepare_model_for_kbit_training</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><b>Model Loading and Configuration</b></td>\n",
    "        <td>bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)<br>model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})</td>\n",
    "        <td>model = AutoModelForCausalLM.from_pretrained(model_path, load_in_low_bit=\"nf4\", optimize_model=False, torch_dtype=torch.float16, modules_to_not_convert=[\"lm_head\"])</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><b>Device Allocation</b></td>\n",
    "        <td># Implicit GPU allocation in device_map for Nvidia A100</td>\n",
    "        <td>model = model.to('xpu')  # Move model to the XPU (PVC 1100)</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d732ddf3-d2d4-4d05-bb41-7c1c1315cd93",
   "metadata": {},
   "source": [
    "**Install the required packages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151f9d94-533a-49cf-bbf4-d61ef192e1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q --pre --upgrade bigdl-llm[xpu] -f https://developer.intel.com/ipex-whl-stable-xpu \n",
    "!{sys.executable} -m pip install -q datasets transformers==4.34.0 peft==0.5.0 accelerate==0.23.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac290f27-a452-46b3-bc30-c576454f866b",
   "metadata": {},
   "source": [
    "**Add locally installed python packages path to `sys.path`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa44a0e-6e92-4811-8c22-4109cce51e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import site\n",
    "from pathlib import Path\n",
    "def get_python_version():\n",
    "     return \"python\" + \".\".join(map(str, sys.version_info[:2]))\n",
    " \n",
    "def set_local_bin_path():\n",
    "     local_bin = str(Path.home() / \".local\" / \"bin\") \n",
    "     local_site_packages = str(\n",
    "         Path.home() / \".local\" / \"lib\" / get_python_version() / \"site-packages\"\n",
    "     )\n",
    "     sys.path.append(local_bin)\n",
    "     sys.path.insert(0, site.getusersitepackages())\n",
    "     sys.path.insert(0, sys.path.pop(sys.path.index(local_site_packages)))\n",
    " \n",
    "set_local_bin_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3d1342-4fde-4dda-aa7a-f7c01322b240",
   "metadata": {},
   "source": [
    "**Required Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189dad14-7fd2-4c6f-b70f-e8e5c860cebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "os.environ[\"NUMEXPR_MAX_THREADS\"] = \"64\"\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=FutureWarning,\n",
    "    message=\"This implementation of AdamW is deprecated\",\n",
    ")\n",
    "\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex  # required for pytorch to identify intel GPU, not needed for nvidia as features are upstream\n",
    "import transformers\n",
    "\n",
    "from transformers import LlamaTokenizer\n",
    "from peft import LoraConfig\n",
    "from bigdl.llm.transformers.qlora import get_peft_model, prepare_model_for_kbit_training  #for nvidia this would be bitsandbytes\n",
    "from bigdl.llm.transformers import AutoModelForCausalLM\n",
    "\n",
    "# for nvidia: from transformers import AutoModelForCausalLM\n",
    "# for nvidia: from peft import get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6f79b7-8301-4589-8f2c-02b871280131",
   "metadata": {},
   "source": [
    "**Model and dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1c6716-3bd6-4e96-ac5a-64f1e244a699",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cache = \"/home/common/data/Big_Data/GenAI/llm_models\"\n",
    "base_model = \"NousResearch--Nous-Hermes-Llama-2-7b\"\n",
    "model_path = os.path.join(model_cache, base_model)\n",
    "dataset = \"Abirate/english_quotes\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "if torch.xpu.is_available():\n",
    "    print(f\"using device: {torch.xpu.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69fa107-8047-430f-905c-b2b712e84af4",
   "metadata": {},
   "source": [
    "**Finetuning code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cd0cbd-26dd-4006-a2d0-9574e6f11fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset)\n",
    "tokenized_dataset = dataset.map(lambda data: tokenizer(data[\"quote\"]), batched=True)\n",
    "\n",
    "# Load model using BigDL in 4 bits, use bitsandbytes config for Nvidia\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                                load_in_low_bit=\"nf4\",\n",
    "                                                optimize_model=False,\n",
    "                                                torch_dtype=torch.float16,\n",
    "                                                modules_to_not_convert=[\"lm_head\"],)\n",
    "\n",
    "model = model.to('xpu')  # move model to the XPU (PVC 1100 here) , for nvidia 'cuda'\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA, same as Nvidia\n",
    "lora_config = LoraConfig(\n",
    "    r=16, \n",
    "    lora_alpha=64, \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"], \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "\n",
    "# Configure trainer, same as Nvidia\n",
    "training_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps= 1,\n",
    "    warmup_steps=20,\n",
    "    max_steps=300,\n",
    "    learning_rate=2e-5,\n",
    "    save_steps=30,\n",
    "    bf16=True,\n",
    "    logging_steps=20,\n",
    "    output_dir=\"outputs\",\n",
    "    optim=\"adamw_hf\",\n",
    ")\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Train model, same as Nvidia\n",
    "training_result = trainer.train()\n",
    "print(training_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcd9a35-1956-4555-9826-ce70bb39dcc2",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d380d4f-935b-4bd4-a7a6-216a246c2740",
   "metadata": {},
   "source": [
    "Let's see how good the model is in completing an English quote."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5084e1da-2867-4c88-89d6-cd186cc09c69",
   "metadata": {},
   "source": [
    "**Using base model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581693eb-ccee-4b4f-8f80-6c64c6e4abc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.llm.transformers.qlora import PeftModel\n",
    "\n",
    "batch = tokenizer(\"A room without books \", return_tensors=\"pt\")\n",
    "\n",
    "# load model in 4 bit\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                             load_in_4bit=True,\n",
    "                                             optimize_model=True,\n",
    "                                             use_cache=True,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             modules_to_not_convert=[\"lm_head\"],)\n",
    "print(\"base model output\\n\", tokenizer.decode(base_model.generate(**batch, max_new_tokens=20)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dd30a5-7713-4d06-9ecb-aee1c4af633b",
   "metadata": {},
   "source": [
    "**Using Finetuned model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea3f834-d54e-4958-9b14-87559a7f1ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = PeftModel.from_pretrained(base_model, \"./outputs/checkpoint-30/\")\n",
    "print(\"finetuned model output\\n\", tokenizer.decode(finetuned_model.generate(**batch, max_new_tokens=20)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f93f73-0d40-4aa9-a655-b2b40379b095",
   "metadata": {},
   "source": [
    "That's it!! 🎉 Now you know how to port a model from Nvidia to Intel GPUs and fine-tune an LLM using QLoRA on your Arc GPUs or Intel Datacenter GPUs on [Intel Developer Cloud](https://cloud.intel.com)! 🚀 As mentioned above, this is a very simple codebase. Please check out the LLM Finetuning notebook at **Gen AI Essentials** on **Intel Developer Cloud** for a practical use case. 📚👨‍💻"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
